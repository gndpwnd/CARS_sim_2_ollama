"""
Streaming endpoints for real-time log updates.
FINAL FIX: Properly tracks point IDs to detect new Qdrant telemetry
"""
import json
import asyncio
from datetime import datetime, timedelta
from fastapi.responses import StreamingResponse

# Import storage backends
try:
    from qdrant_store import qdrant_client, TELEMETRY_COLLECTION
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False
    qdrant_client = None

try:
    from postgresql_store import get_messages_by_type
    POSTGRESQL_AVAILABLE = True
except ImportError:
    POSTGRESQL_AVAILABLE = False


def normalize_timestamp_to_iso(ts: any) -> str:
    """
    Normalize any timestamp format to ISO string.
    Handles: Unix timestamps (float/int), ISO strings, datetime objects.
    """
    if isinstance(ts, str):
        # Already a string, just return it
        return ts
    elif isinstance(ts, (int, float)):
        # Unix timestamp - convert to ISO
        try:
            return datetime.fromtimestamp(ts).isoformat()
        except (ValueError, OSError):
            # Invalid timestamp, use current time
            return datetime.now().isoformat()
    elif isinstance(ts, datetime):
        return ts.isoformat()
    else:
        # Unknown type, use current time
        return datetime.now().isoformat()


def fetch_logs_from_qdrant(limit=200, since_point_ids=None):
    """
    Fetch logs from Qdrant - FIXED to return point IDs for tracking
    
    Args:
        limit: Maximum number of logs to fetch
        since_point_ids: Set of point IDs already seen (to skip)
    
    Returns:
        List of log dictionaries with point IDs
    """
    if not QDRANT_AVAILABLE or not qdrant_client:
        return []
    
    if since_point_ids is None:
        since_point_ids = set()
    
    try:
        # Scroll through the collection
        results = qdrant_client.scroll(
            collection_name=TELEMETRY_COLLECTION,
            limit=limit,
            with_payload=True,
            with_vectors=False
        )[0]
        
        logs = []
        for point in results:
            point_id = str(point.id)
            
            # Skip if we've already sent this point
            if point_id in since_point_ids:
                continue
            
            payload = point.payload
            
            # Normalize timestamp to ISO format
            raw_timestamp = payload.get('timestamp', datetime.now())
            iso_timestamp = normalize_timestamp_to_iso(raw_timestamp)
            
            # Build log entry
            log = {
                '_point_id': point_id,  # Include point ID for tracking
                'log_id': point_id[:8],  # Short ID for display
                'text': payload.get('text', ''),
                'metadata': {
                    'agent_id': payload.get('agent_id'),
                    'position': payload.get('position'),
                    'jammed': payload.get('jammed', False),
                    'communication_quality': payload.get('communication_quality', 0),
                    'iteration': payload.get('iteration'),
                    'timestamp': iso_timestamp
                },
                'created_at': iso_timestamp,
                'source': 'qdrant'
            }
            
            logs.append(log)
        
        return logs
        
    except Exception as e:
        print(f"[STREAMING] Error fetching from Qdrant: {e}")
        import traceback
        traceback.print_exc()
        return []


async def stream_qdrant():
    """
    Stream Qdrant logs in real-time - FIXED with proper point ID tracking
    """
    async def event_generator():
        # Track which point IDs we've already sent
        seen_point_ids = set()
        check_count = 0
        initial_load_done = False
        
        print("[STREAMING] Qdrant stream generator started")
        
        while True:
            try:
                check_count += 1
                
                # Fetch logs (exclude already-seen point IDs)
                logs = fetch_logs_from_qdrant(limit=200, since_point_ids=seen_point_ids)
                
                # Filter to only NEW logs (belt-and-suspenders with the since_point_ids filter)
                new_logs = [
                    log for log in logs 
                    if log.get('_point_id') not in seen_point_ids
                ]
                
                # DEBUG logging
                if check_count == 1 or check_count % 20 == 0:
                    print(f"[STREAMING] Qdrant check #{check_count}: "
                          f"fetched {len(logs)} total logs, "
                          f"{len(new_logs)} new logs, "
                          f"seen {len(seen_point_ids)} point IDs")
                
                # Send new logs to frontend
                if new_logs:
                    for log in new_logs:
                        # Add point ID to seen set BEFORE sending
                        point_id = log.get('_point_id')
                        if point_id:
                            seen_point_ids.add(point_id)
                        
                        # Remove internal fields before sending
                        log_copy = log.copy()
                        log_copy.pop('_point_id', None)
                        
                        # Send to frontend
                        yield f"data: {json.dumps(log_copy)}\n\n"
                    
                    print(f"[STREAMING] Qdrant: SENT {len(new_logs)} new logs to frontend "
                          f"(total seen: {len(seen_point_ids)})")
                    
                    # Mark initial load as complete
                    if not initial_load_done:
                        initial_load_done = True
                        print(f"[STREAMING] Qdrant: Initial load complete ({len(new_logs)} logs)")
                
                # Wait before checking again (0.5 seconds for responsiveness)
                await asyncio.sleep(0.5)
                
            except asyncio.CancelledError:
                print("[STREAMING] Qdrant stream cancelled by client")
                break
            except Exception as e:
                print(f"[STREAMING] Error in Qdrant stream: {e}")
                import traceback
                traceback.print_exc()
                await asyncio.sleep(1)  # Wait before retrying
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"
        }
    )


async def stream_postgresql():
    """
    Stream PostgreSQL logs in real-time.
    """
    async def event_generator():
        seen_ids = set()
        check_count = 0
        
        print("[STREAMING] PostgreSQL stream generator started")
        
        while True:
            try:
                check_count += 1
                
                # Get recent messages
                messages = []
                if POSTGRESQL_AVAILABLE:
                    try:
                        # Get user messages
                        user_msgs = get_messages_by_type('command', limit=50)
                        # Get LLM responses
                        llm_msgs = get_messages_by_type('response', limit=50)
                        # Get errors
                        error_msgs = get_messages_by_type('error', limit=50)
                        # Get notifications
                        notif_msgs = get_messages_by_type('notification', limit=50)
                        
                        messages = user_msgs + llm_msgs + error_msgs + notif_msgs
                    except Exception as e:
                        print(f"[STREAMING] Error fetching PostgreSQL messages: {e}")
                
                # Filter to new messages
                new_messages = [
                    msg for msg in messages 
                    if str(msg['id']) not in seen_ids
                ]
                
                # DEBUG logging
                if check_count == 1 or check_count % 20 == 0:
                    print(f"[STREAMING] PostgreSQL check #{check_count}: "
                          f"fetched {len(messages)} total, "
                          f"{len(new_messages)} new, "
                          f"seen {len(seen_ids)} IDs")
                
                # Send new messages
                for msg in new_messages:
                    msg_id = str(msg['id'])
                    seen_ids.add(msg_id)
                    
                    # Format for frontend
                    log_entry = {
                        'log_id': msg_id[:8],
                        'text': msg['text'],
                        'metadata': msg.get('metadata', {}),
                        'created_at': msg.get('created_at', datetime.now()).isoformat() 
                                     if isinstance(msg.get('created_at'), datetime) 
                                     else str(msg.get('created_at', '')),
                        'source': 'postgresql'
                    }
                    
                    yield f"data: {json.dumps(log_entry)}\n\n"
                
                if new_messages:
                    print(f"[STREAMING] PostgreSQL: SENT {len(new_messages)} new messages")
                
                await asyncio.sleep(0.5)
                
            except asyncio.CancelledError:
                print("[STREAMING] PostgreSQL stream cancelled")
                break
            except Exception as e:
                print(f"[STREAMING] Error in PostgreSQL stream: {e}")
                await asyncio.sleep(1)
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"
        }
    )